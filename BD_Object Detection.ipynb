{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clones the githup repository 'yolov5', and installs all the requirements in the req. text file. Will have to research this repository too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5  # clone\n",
    "%cd yolov5\n",
    "%pip install -qr requirements.txt  # install\n",
    "\n",
    "import torch\n",
    "from yolov5 import utils\n",
    "display = utils.notebook_init()  # checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only to be used in google colab, most definitely doesn't work in normal environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the files are hosted in their gdrive, and are being loaded into colab. So, all the cleanup and other stuff are either being done in another notebook, or it is already pre processed. It is fine, but we can easily include that cuz I already have the code for cleanup and preprocessing in my other repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path = \"/content/drive/MyDrive/STATS402_Project/hateful_memes/\"\n",
    "dev_seen_data= pd.read_json(path+'dev_seen.jsonl',lines=True)\n",
    "dev_unseen_data= pd.read_json(path+'dev_unseen.jsonl',lines=True)\n",
    "test_seen_data=pd.read_json(path+'test_seen.jsonl',lines=True)\n",
    "test_unseen_data=pd.read_json(path+'test_unseen.jsonl',lines=True)\n",
    "train_data=pd.read_json(path+'train.jsonl',lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatinating (Adding) all the datasets into one for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train_data,test_seen_data,test_unseen_data,dev_seen_data,dev_unseen_data])\n",
    "data1 = data.drop_duplicates(subset=['img'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet another path. Just use fucking github repository and link it directly, fuckin buffoons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2 = '/content/drive/MyDrive/STATS402_Project/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an array(?) in which all the detected objects are loaded through detection through pytorch and yolov5 repository. Need to verify if this even works or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = {}\n",
    "for each in data1['img']:\n",
    "  print(each)\n",
    "  img_path = path2 + each\n",
    "  model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "  results = model(img_path)\n",
    "  object1 = results.pandas().xyxy[0]['name'].unique()\n",
    "  print(object1)\n",
    "  objects[each] = object1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aaand just checking the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
